  version: '3'

  services:
    zoo1:
      image: confluentinc/cp-zookeeper:7.3.2
      ports:
        - "2181:2181"
      environment:
        ZOOKEEPER_CLIENT_PORT: 2181
        ZOOKEEPER_SERVER_ID: 1
        ZOOKEEPER_SERVERS: zoo1:2888:3888
      networks:
        - kafka-network


    kafka1:
      image: confluentinc/cp-kafka:7.3.2
      container_name: kafka1
      ports:
        - "9092:9092"
        - "29092:29092"
      environment:
        KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
        KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
        KAFKA_BROKER_ID: 1
        KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
        KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
        KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
      networks:
        - kafka-network


    kafka2:
      image: confluentinc/cp-kafka:7.3.2
      ports:
        - "9093:9093"
        - "29093:29093"
      environment:
        KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:19093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093,DOCKER://host.docker.internal:29093
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
        KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
        KAFKA_BROKER_ID: 2
        KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
        KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
        KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
      networks:
        - kafka-network


    kafka3:
      image: confluentinc/cp-kafka:7.3.2
      ports:
        - "9094:9094"
        - "29094:29094"
      environment:
        KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka3:19094,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094,DOCKER://host.docker.internal:29094
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
        KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
        KAFKA_BROKER_ID: 3
        KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
        KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
        KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
      networks:
        - kafka-network


    kafka-connect:
      image: confluentinc/cp-kafka-connect:7.3.2
      ports:
        - "8083:8083"
      environment:
        CONNECT_BOOTSTRAP_SERVERS: kafka1:19092,kafka2:19093,kafka3:19094
        CONNECT_REST_PORT: 8083
        CONNECT_GROUP_ID: compose-connect-group
        CONNECT_CONFIG_STORAGE_TOPIC: compose-connect-configs
        CONNECT_OFFSET_STORAGE_TOPIC: compose-connect-offsets
        CONNECT_STATUS_STORAGE_TOPIC: compose-connect-status
        CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
        CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
        CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
        CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
        CONNECT_REST_ADVERTISED_HOST_NAME: 'kafka-connect'
        CONNECT_LOG4J_ROOT_LOGLEVEL: 'INFO'
        CONNECT_LOG4J_LOGGERS: 'org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR'
        CONNECT_PLUGIN_PATH: '/usr/share/java,/usr/share/confluent-hub-components'
      networks:
        - kafka-network


    schema-registry:
      image: confluentinc/cp-schema-registry:7.3.2
      ports:
        - "8081:8081"
      environment:
        SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka1:19092,kafka2:19093,kafka3:19094
        SCHEMA_REGISTRY_HOST_NAME: schema-registry
        SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      networks:
        - kafka-network


    kafka-ui:
      container_name: kafka-ui-1
      image: provectuslabs/kafka-ui:latest
      ports:
        - 8888:8080 # Changed to avoid port clash with akhq
      depends_on:
        - kafka1
        - kafka2
        - kafka3
        - schema-registry
        - kafka-connect
      environment:
        KAFKA_CLUSTERS_0_NAME: local
        KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: PLAINTEXT://kafka1:19092,PLAINTEXT_HOST://kafka1:19092
        KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
        KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: connect
        KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083
        DYNAMIC_CONFIG_ENABLED: 'true'
      networks:
        - kafka-network
        


    # spark-master:
    #   image: bitnami/spark:3
    #   container_name: spark_master
    #   ports:
    #     - 8085:8080
    #   environment:
    #     - SPARK_UI_PORT=8080
    #     - SPARK_MODE=master
    #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
    #     - SPARK_RPC_ENCRYPTION_ENABLED=no
    #   volumes:
    #     - ./:/home
    #     - spark-data:/opt/bitnami/spark/data
    #   networks:
    #     - airflow-kafka
    #     - kafka-network
    # spark-master:
    #     image: bitnami/spark:latest
    #     container_name: spark_master
    #     command: bin/spark-class org.apache.spark.deploy.master.Master
    #     ports:
    #       - "9090:8080"
    #       - "7077:7077"
    #     volumes:
    #       - ./:/home
    #       - spark-data:/opt/bitnami/spark/data
    #     networks:
    #       - airflow-kafka
    #       - kafka-network
    # spark-worker:
    #     image: bitnami/spark:latest
    #     command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    #     depends_on:
    #       - spark-master
    #     environment:
    #       SPARK_MODE: worker
    #       SPARK_WORKER_CORES: 2
    #       SPARK_WORKER_MEMORY: 1g
    #       SPARK_MASTER_URL: spark://spark-master:7077
    #     networks:
    #       - airflow-kafka
    #       - kafka-network
    
    
    spark-master:
      image: our-own-apache-spark:3.4.0
      container_name: 'spark-master'
      ports:
        - "9090:8080"
        - "7077:7077"
      volumes:
        - ./apps:/opt/spark-apps
        - ./data:/opt/spark-data
      environment:
        - SPARK_LOCAL_IP=spark-master
        - SPARK_WORKLOAD=master
      networks:
        - kafka-network
    spark-worker-a:
      image: our-own-apache-spark:3.4.0
      ports:
        - "9091:8080"
        - "7000:7000"
      depends_on:
        - spark-master
      environment:
        - SPARK_MASTER=spark://spark-master:7077
        - SPARK_WORKER_CORES=1
        - SPARK_WORKER_MEMORY=1G
        - SPARK_DRIVER_MEMORY=1G
        - SPARK_EXECUTOR_MEMORY=1G
        - SPARK_WORKLOAD=worker
        - SPARK_LOCAL_IP=spark-worker-a
      volumes:
        - ./apps:/opt/spark-apps
        - ./data:/opt/spark-data
      networks:
        - kafka-network
    spark-worker-b:
      image: our-own-apache-spark:3.4.0
      ports:
        - "1001:8080"
        - "7001:7000"
      depends_on:
        - spark-master
      environment:
        - SPARK_MASTER=spark://spark-master:7077
        - SPARK_WORKER_CORES=1
        - SPARK_WORKER_MEMORY=1G
        - SPARK_DRIVER_MEMORY=1G
        - SPARK_EXECUTOR_MEMORY=1G
        - SPARK_WORKLOAD=worker
        - SPARK_LOCAL_IP=spark-worker-b
      volumes:
          - ./apps:/opt/spark-apps
          - ./data:/opt/spark-data 
      networks:
        - kafka-network
      
    
    
    
    cassandra:
        image: cassandra:latest
        container_name: cassandra
        hostname: cassandra
        ports:
          - 9042:9042
        environment:
          - MAX_HEAP_SIZE=512M
          - HEAP_NEWSIZE=100M
          - CASSANDRA_USERNAME=cassandra
          - CASSANDRA_PASSWORD=cassandra
        volumes:
          - ./:/home
          - cassandra-data:/var/lib/cassandra
        networks:
          - kafka-network  
    webserver:
        image: apache/airflow:2.6.0-python3.9
        command: webserver
        entrypoint: ['/bin/bash', '-c', '/opt/airflow/script/entrypoint.sh']
        depends_on:
          - postgres
        environment:
          - LOAD_EX=n
          - EXECUTOR=Sequential
          - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
          - AIRFLOW_WEBSERVER_SECRET_KEY=this_is_a_very_secured_key
        logging:
          options:
            max-size: 10m
            max-file: "3"
        volumes:
          - ./dags:/opt/airflow/dags
          - ./script/entrypoint.sh:/opt/airflow/script/entrypoint.sh
          - ./requirements.txt:/opt/airflow/requirements.txt
        ports:
          - "8080:8080"
        healthcheck:
          #test: ['CMD-SHELL', "[ -f /opt/airflow/airflow-webserver.pid ]"]
          test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ] || exit 1"]
          interval: 30s
          timeout: 30s
          retries: 3
          start_period: 600s
        networks:
          - kafka-network   
          - airflow-kafka
    scheduler:
        image: apache/airflow:2.6.0-python3.9
        depends_on:
          webserver:
            condition: service_healthy
        volumes:
          - ./dags:/opt/airflow/dags
          - ./script/entrypoint.sh:/opt/airflow/script/entrypoint.sh
          - ./requirements.txt:/opt/airflow/requirements.txt
        environment:
          - LOAD_EX=n
          - EXECUTOR=Sequential
          - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
          - AIRFLOW_WEBSERVER_SECRET_KEY=this_is_a_very_secured_key
        command: bash -c "pip install -r ./requirements.txt && airflow db upgrade && airflow scheduler"
        networks:
          - kafka-network
          - airflow-kafka
    postgres:
        image: postgres:14.0
        environment:
          - POSTGRES_USER=airflow
          - POSTGRES_PASSWORD=airflow
          - POSTGRES_DB=airflow
        logging:
          options:
            max-size: 10m
            max-file: "3"
        networks:
        - kafka-network  
  volumes:
      cassandra-data:
      spark-data:

  networks:
    kafka-network:
      driver: bridge
    airflow-kafka:
      external: true
    


      